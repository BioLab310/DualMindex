/*
 * prealignment.cpp
 *
 *  Created on: May 25, 2020
 *      Author: pluto
 */

#include "prealignment.h"

char MASKA_16[16] __aligned__ = { 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',
		'A', 'A', 'A', 'A', 'A', 'A', 'A' };

char MASKC_16[16] __aligned__ = { 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C',
		'C', 'C', 'C', 'C', 'C', 'C', 'C' };

char MASKG_16[16] __aligned__ = { 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G',
		'G', 'G', 'G', 'G', 'G', 'G', 'G' };

char MASKT_16[16] __aligned__ = { 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T',
		'T', 'T', 'T', 'T', 'T', 'T', 'T' };

uint8_t BASE_SHIFT1[16] __aligned__ = { 0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13,
		3, 11, 7, 15 };

uint8_t BASE_SHIFT2[16] __aligned__ = { 0, 1, 8, 9, 4, 5, 12, 13, 2, 3, 10, 11,
		6, 7, 14, 15 };

uint8_t BIT_FF[16] __aligned__ = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
		0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };

uint8_t LOC_MASK1[128] = { 0x01, 0x01, 0x01, 0x01, //1
		0x01, 0x01, 0x01, 0x01, //1
		0x01, 0x01, 0x01, 0x01, //1
		0x01, 0x01, 0x01, 0x01, //1
		0x04, 0x04, 0x04, 0x04, //3
		0x04, 0x04, 0x04, 0x04, //3
		0x04, 0x04, 0x04, 0x04, //3
		0x04, 0x04, 0x04, 0x04, //3
		0x02, 0x02, 0x02, 0x02, //2
		0x02, 0x02, 0x02, 0x02, //2
		0x02, 0x02, 0x02, 0x02, //2
		0x02, 0x02, 0x02, 0x02, //2
		0x08, 0x08, 0x08, 0x08, //4
		0x08, 0x08, 0x08, 0x08, //4
		0x08, 0x08, 0x08, 0x08, //4
		0x08, 0x08, 0x08, 0x08, //4
		0x10, 0x10, 0x10, 0x10, //5
		0x10, 0x10, 0x10, 0x10, //5
		0x10, 0x10, 0x10, 0x10, //5
		0x10, 0x10, 0x10, 0x10, //5
		0x40, 0x40, 0x40, 0x40, //7
		0x40, 0x40, 0x40, 0x40, //7
		0x40, 0x40, 0x40, 0x40, //7
		0x40, 0x40, 0x40, 0x40, //7
		0x20, 0x20, 0x20, 0x20, //6
		0x20, 0x20, 0x20, 0x20, //6
		0x20, 0x20, 0x20, 0x20, //6
		0x20, 0x20, 0x20, 0x20, //6
		0x80, 0x80, 0x80, 0x80, //8
		0x80, 0x80, 0x80, 0x80, //8
		0x80, 0x80, 0x80, 0x80, //8
		0x80, 0x80, 0x80, 0x80 //8
		};

uint8_t __MASK_SSE_BEG1_ [128] __aligned__ = { 0xfe, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfc,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xf8, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf0, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xe0,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xc0, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x80, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00,
	0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
	0xff, 0xff, 0xff };

uint8_t __MASK_SSE_END1_ [SSE_BIT_LENGTH * SSE_BYTE_NUM / BASE_SIZE1] = {

0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x03,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x07,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x0f,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x1f,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x3f,
0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f

};

uint8_t read_vec0_t[SSE_BYTE_NUM] __aligned__;
uint8_t read_vec1_t[SSE_BYTE_NUM] __aligned__;
uint8_t ref_vec0_t[SSE_BYTE_NUM] __aligned__;
uint8_t ref_vec1_t[SSE_BYTE_NUM] __aligned__;
uint8_t* MASK_SSE_BEG1 = __MASK_SSE_BEG1_;
uint8_t* MASK_SSE_END1 = __MASK_SSE_END1_;

//

uint32_t popcnt_sse_u64(__m128i reg){
	uint32_t result = 0;
	result += _mm_popcnt_u64(*(uint64_t *)&reg);
	result += _mm_popcnt_u64(*((uint64_t *)&reg + 1));
	return result;
}

__m128i mask_generate(int length)
{
	__m128i mask;
	if (length >= SSE_BASE_NUM1)
			mask = _mm_set1_epi8(0xff);
		else
			mask = _mm_load_si128( (__m128i *) (MASK_SSE_END1 + (length *
						SSE_BYTE_NUM)));
	return mask;
}

__m128i shift_left_sse1(__m128i vec, int shift_num) {
	if(shift_num == 8)
		return _mm_srli_si128(vec, 1);
	__m128i carryover = _mm_srli_si128(vec, 1);
	carryover = _mm_slli_epi64(carryover, 8 - (shift_num % 8));
	vec = _mm_srli_epi64(vec, shift_num % 8);
	return _mm_or_si128(vec, carryover);
}

__m128i shift_left_si128(__m128i vec, int shift_num) {
	__m128i right_reg = _mm_srli_si128(vec, 8);
	if(shift_num >= 64)
		return _mm_srli_epi64(right_reg, shift_num-64);
	__m128i carryover = _mm_srli_epi64(vec, shift_num);
	right_reg = _mm_slli_epi64(right_reg, 64-shift_num);
	return _mm_or_si128(carryover, right_reg);
}

__m128i shift_right_sse1(__m128i vec, int shift_num) {
	if(shift_num == 8)
		return _mm_slli_si128(vec, 1);
	__m128i carryover = _mm_slli_si128(vec, 1);
	carryover = _mm_srli_epi64(carryover, 8 - (shift_num % 8));
	vec = _mm_slli_epi64(vec, shift_num % 8);
	return _mm_or_si128(vec, carryover);
}

void sse3_convert2bit1(char *str, uint8_t *bits0, uint8_t *bits1) {

	__m128i *shift_hint = (__m128i *) BASE_SHIFT1;
	__m128i *cast_str;

	__m128i temp;
	__m128i result0, result1;

	//Mask for bit0 and bit1
	__m128i *maskA = (__m128i *) MASKA_16;
	__m128i *maskC = (__m128i *) MASKC_16;
	__m128i *maskG = (__m128i *) MASKG_16;
	__m128i *maskT = (__m128i *) MASKT_16;
	__m128i *mask;

	int i;

	for (i = 0; i < 128; i += 16) {
		cast_str = (__m128i *) (str + i);
		*cast_str = _mm_shuffle_epi8(*cast_str, *shift_hint);
	}

	for (i = 8; i < 128; i += 32) {
		cast_str = (__m128i *) (str + i);
		temp = _mm_loadu_si128(cast_str);
		temp = _mm_shuffle_epi32(temp, 0x4e);
		_mm_storeu_si128(cast_str, temp);
	}

	shift_hint = (__m128i *) BASE_SHIFT2;
	for (i = 0; i < 128; i += 16) {
		cast_str = (__m128i *) (str + i);
		*cast_str = _mm_shuffle_epi8(*cast_str, *shift_hint);
	}

	for (i = 16; i < 128; i += 64) {
		temp = *((__m128i *) (str + i));
		*((__m128i *) (str + i)) = *((__m128i *) (str + i + 16));
		*((__m128i *) (str + i + 16)) = temp;
	}

	for (i = 8; i < 128; i += 32) {
		cast_str = (__m128i *) (str + i);
		temp = _mm_loadu_si128(cast_str);
		temp = _mm_shuffle_epi32(temp, 0x4e);
		_mm_storeu_si128(cast_str, temp);
	}

	for (i = 0; i < 128; i += 16) {
		cast_str = (__m128i *) (str + i);
		temp = _mm_load_si128(cast_str);
		temp = _mm_shuffle_epi32(temp, 0xd8);
		_mm_store_si128(cast_str, temp);
	}

	for (i = 16; i < 64; i += 32) {
		temp = *((__m128i *) (str + i));
		*((__m128i *) (str + i)) = *((__m128i *) (str + i + 48));
		*((__m128i *) (str + i + 48)) = temp;
	}

	for (i = 8; i < 128; i += 32) {
		cast_str = (__m128i *) (str + i);
		temp = _mm_loadu_si128(cast_str);
		temp = _mm_shuffle_epi32(temp, 0x4e);
		_mm_storeu_si128(cast_str, temp);
	}

	result0 = _mm_set1_epi32(0);
	result1 = _mm_set1_epi32(0);
	__m128i* bit0_reg = (__m128i*) bits0;
	__m128i* bit1_reg = (__m128i*) bits1;

	*bit0_reg = _mm_set1_epi32(0);
	*bit1_reg = _mm_set1_epi32(0);

	for (i = 0; i < 128; i += 16) {
		cast_str = (__m128i *) (str + i);
		temp = _mm_cmpeq_epi8(*maskC, *cast_str);
		result0 = _mm_and_si128(temp, *((__m128i *) BIT_FF)); //C=01

		temp = _mm_cmpeq_epi8(*maskG, *cast_str);
		temp = _mm_and_si128(temp, *((__m128i *) BIT_FF));
		result1 = _mm_or_si128(result1, temp); //G=10

		temp = _mm_cmpeq_epi8(*maskT, *cast_str);
		temp = _mm_and_si128(temp, *((__m128i *) BIT_FF)); //T=11
		result0 = _mm_or_si128(result0, temp); //T=11
		result1 = _mm_or_si128(result1, temp); //T=11

		mask = (__m128i *) (LOC_MASK1 + i);

		result0 = _mm_and_si128(*mask, result0);
		result1 = _mm_and_si128(*mask, result1);
		*bit0_reg = _mm_or_si128(*bit0_reg, result0);
		*bit1_reg = _mm_or_si128(*bit1_reg, result1);

	}
}

int banded_edit_distance(const char *pattern, const char *text, \
		int text_length, const int error_threshold,int * mapping_end_posi) {
  uint32_t Peq[5] = {0, 0, 0, 0, 0};
  for (int i = 0; i < 2 * error_threshold; i++) {
    uint8_t base = char_to_uint8(pattern[i]);
    Peq[base] = Peq[base] | (1 << i);
  }
  uint32_t highest_bit_in_band_mask = 1 << (2 * error_threshold);
  uint32_t lowest_bit_in_band_mask = 1;
  uint32_t VP = 0;
  uint32_t VN = 0;
  uint32_t X = 0;
  uint32_t D0 = 0;
  uint32_t HN = 0;
  uint32_t HP = 0;
  int num_errors_at_band_start_position = 0;
  for (int i = 0; i < text_length; i++) {
    uint8_t pattern_base = char_to_uint8(pattern[i + 2 * error_threshold]);
    Peq[pattern_base] = Peq[pattern_base] | highest_bit_in_band_mask;
    X = Peq[char_to_uint8(text[i])] | VN;
    D0 = ((VP + (X & VP)) ^ VP) | X;
    HN = VP & D0;
    HP = VN | ~(VP | D0);
    X = D0 >> 1;
    VN = X & HP;
    VP = HN | ~(X | HP);
    num_errors_at_band_start_position += 1 - (D0 & lowest_bit_in_band_mask);
    if (num_errors_at_band_start_position > 3 * error_threshold) {
      return error_threshold + 1;
    }
    for (int ai = 0; ai < 5; ai++) {
      Peq[ai] >>= 1;
    }
  }
  int band_start_position = text_length - 1;
  int min_num_errors = num_errors_at_band_start_position;
  *mapping_end_posi=band_start_position;
  for (int i = 0; i < 2 * error_threshold; i++) {
    num_errors_at_band_start_position = num_errors_at_band_start_position + ((VP >> i) & (uint32_t) 1);
    num_errors_at_band_start_position = num_errors_at_band_start_position - ((VN >> i) & (uint32_t) 1);
    if (num_errors_at_band_start_position < min_num_errors) {
      min_num_errors = num_errors_at_band_start_position;
      *mapping_end_posi=band_start_position+1+i;
    }
  }
  return min_num_errors;
}

int pre_alignment(char *read, uint32_t read_len, char *ref, uint32_t start, uint32_t end, uint32_t max_error)
{
	sse3_convert2bit1(read, read_vec0_t, read_vec1_t);
	sse3_convert2bit1(ref/*+start-max_error*/, ref_vec0_t, ref_vec1_t);
	//sse3_convert2bit1(ref/*+start-max_error*/, ref_vec0_t, ref_vec1_t);  //work!

//	__m128i read_XMM0 = *((__m128i *) (read_vec0_t));
//	__m128i read_XMM1 = *((__m128i *) (read_vec1_t));
//
//	//ref data
//	__m128i ref_XMM0 = *((__m128i *) (ref_vec0_t));
//	__m128i ref_XMM1 = *((__m128i *) (ref_vec1_t));
//
//	uint32_t vec_len = end-start+1+2*max_error;
//	__m128i shift_XMM;
////	__m128i shift_XMMA[vec_len];
//	__m128i *shift_XMMA = new __m128i[vec_len];
//	__m128i temp_diff_XMM;
//	__m128i temp_shift_XMM;
//	__m128i mask_end;
//
//	mask_end = mask_generate(read_len);
//	for(uint32_t i = 0; i < vec_len; ++i)
//	{
//		shift_XMM = shift_left_si128(ref_XMM0, i);
//		temp_diff_XMM = _mm_xor_si128(shift_XMM, read_XMM0);
//		shift_XMM = shift_left_si128(ref_XMM1, i);
//		temp_shift_XMM = _mm_xor_si128(shift_XMM, read_XMM1);
//		temp_diff_XMM = _mm_or_si128(temp_shift_XMM, temp_diff_XMM);
//
//		shift_XMMA[i] = _mm_and_si128(temp_diff_XMM, mask_end);
//	}
//	uint32_t _cnt1 = 0;
//	for(uint32_t t = 0; t <= max_error; ++t)
//	{
//		for(uint32_t i = 0; i < end-start+1+t; ++i)
//		{
//			if(t == 0)
//			{
//				_cnt1 = popcnt_sse_u64(shift_XMMA[i+max_error]);
////				print128_bit(shift_XMMA[i+max_error]);
//			}
//			else
//			{
//				if(i == end-start+t)
//				{
//					shift_XMMA[i+max_error-t] = _mm_and_si128(shift_XMMA[i+max_error-t], shift_XMMA[i+max_error]);
//				}
//				else
//				{
//					shift_XMMA[i+max_error-t] = _mm_and_si128(shift_XMMA[i+max_error-t], shift_XMMA[i+max_error-t+1]);
//				}
//				_cnt1 = popcnt_sse_u64(shift_XMMA[i+max_error-t]);
////				print128_bit(shift_XMMA[i+max_error-t]);
//			}
//			if(_cnt1 <= max_error - t)
//			{
//				delete [] shift_XMMA;
//				return 1;
//			}
//		}
//	}
//	delete [] shift_XMMA;
	return 0;
}

int bit_vec_filter_m128_sse1_ex(uint8_t *read_vec0, uint8_t *read_vec1, uint8_t
				*ref_vec0, uint8_t *ref_vec1, int read_len, int ref_len, int max_error) {

	int len_sub=read_len-ref_len;
	if(len_sub > max_error || len_sub < -max_error)
	{
		return 0;
	}

	int relative_error;
	int total_difference = 0;

	//Start iteration
	int j;
	//read data
	__m128i read_XMM0 = *((__m128i *) (read_vec0));
	__m128i read_XMM1 = *((__m128i *) (read_vec1));
	//ref data
	__m128i ref_XMM0 = *((__m128i *) (ref_vec0));
	__m128i ref_XMM1 = *((__m128i *) (ref_vec1));

	__m128i shift_XMM;
	__m128i diff_XMM;
	__m128i temp_diff_XMM;
	__m128i temp_shift_XMM;
	__m128i temp_mask;
	__m128i mask_end;

	if(len_sub > 0)
	{
		mask_end = 	mask_generate(ref_len);
	}
	else
	{
		mask_end = mask_generate(read_len);
	}
	diff_XMM = _mm_xor_si128(read_XMM0, ref_XMM0);
	temp_diff_XMM = _mm_xor_si128(read_XMM1, ref_XMM1);
	diff_XMM = _mm_or_si128(diff_XMM, temp_diff_XMM);
	diff_XMM = _mm_and_si128(diff_XMM, mask_end);

	//flip_false_zero(diff_XMM);
	diff_XMM = _mm_and_si128(diff_XMM, mask_end);//11

//	printf("diff_XMM: \t");
//	print128_bit(diff_XMM);

	int r0_l=0;
	int r0_h=0;

	if(len_sub>0)
	{
		r0_l=0;
		r0_h=len_sub;
		for(int i=1;i<=len_sub;i++)
		{
			temp_mask = _mm_load_si128( (__m128i *) (MASK_SSE_BEG1 + (i - 1) *
											SSE_BYTE_NUM));

			mask_end=mask_generate(ref_len+i);
			temp_mask = _mm_and_si128(temp_mask, mask_end);
			shift_XMM = shift_right_sse1(ref_XMM0, i);
			temp_diff_XMM = _mm_xor_si128(shift_XMM, read_XMM0);
			shift_XMM = shift_right_sse1(ref_XMM1, i);
			temp_shift_XMM = _mm_xor_si128(shift_XMM, read_XMM1);
			temp_diff_XMM = _mm_or_si128(temp_shift_XMM, temp_diff_XMM);

			temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);
	//		printf("Before flip: \t");
	//		print128_bit(temp_diff_XMM);
			//flip_false_zero(temp_diff_XMM);
			temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);//11
			diff_XMM = _mm_and_si128(diff_XMM, temp_diff_XMM);
		}
	}
	else if(len_sub<0)
	{
		r0_l=-len_sub;
		r0_h=0;

		temp_mask=mask_generate(read_len);
		for(int i=-1;i>=len_sub;i--)
		{
			shift_XMM = shift_left_sse1(ref_XMM0, -i);
			temp_diff_XMM = _mm_xor_si128(shift_XMM, read_XMM0);
			shift_XMM = shift_left_sse1(ref_XMM1, -i);
			temp_shift_XMM = _mm_xor_si128(shift_XMM, read_XMM1);
			temp_diff_XMM = _mm_or_si128(temp_shift_XMM, temp_diff_XMM);

			temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);
	//		printf("Before flip: \t");
	//		print128_bit(temp_diff_XMM);
			//flip_false_zero(temp_diff_XMM);
			temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);//11
			diff_XMM = _mm_and_si128(diff_XMM, temp_diff_XMM);
		}
	}
	total_difference = popcnt_sse_u64(diff_XMM);
	if(total_difference<=max_error)
	{
		return 1;
	}
	else
	{
		//diff_XMM
		//r0_l,r0_h
		//relative_error

		__m128i up_layer[10];
		__m128i cur_layer[10];
		__m128i * p_up;
		__m128i * p_cur;
		__m128i * p_swap;

		p_up=up_layer;
		p_cur=cur_layer;
		if(len_sub>0)
		{
			relative_error=max_error-len_sub;
		}
		else
		{
			relative_error=max_error+len_sub;
		}

		up_layer[0]=diff_XMM;
		for(int i=1;i<=relative_error/2;i++)
		{
			if(max_error == 1)
			{
				printf("error from ex max_err == 1\n");
			}

			for(j=0;j<=i;j++)
			{
				if(j==0)
				{
					if(len_sub>0)
					{
						temp_mask = mask_generate(ref_len-i);
					}
					else
					{
						temp_mask = mask_generate(read_len-i);//-i
					}
					shift_XMM = shift_left_sse1(ref_XMM0, r0_l+i);
					temp_diff_XMM = _mm_xor_si128(shift_XMM, read_XMM0);
					shift_XMM = shift_left_sse1(ref_XMM1, r0_l+i);
					temp_shift_XMM = _mm_xor_si128(shift_XMM, read_XMM1);
					temp_diff_XMM = _mm_or_si128(temp_shift_XMM, temp_diff_XMM);
					temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);
					//flip_false_zero(temp_diff_XMM);
					temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);//11
					p_cur[j] = _mm_and_si128(p_up[j], temp_diff_XMM);
					//print128_bit(p_cur[j]);
				}
				else if(j==i)
				{
					temp_mask = _mm_load_si128( (__m128i *) (MASK_SSE_BEG1 + (r0_h+i - 1) *
																SSE_BYTE_NUM));
					__m128i mask_end=mask_generate(read_len);
					temp_mask = _mm_and_si128(temp_mask, mask_end);

					shift_XMM = shift_right_sse1(ref_XMM0, r0_h+i);
					temp_diff_XMM = _mm_xor_si128(shift_XMM, read_XMM0);
					shift_XMM = shift_right_sse1(ref_XMM1, r0_h+i);
					temp_shift_XMM = _mm_xor_si128(shift_XMM, read_XMM1);
					temp_diff_XMM = _mm_or_si128(temp_shift_XMM, temp_diff_XMM);
					temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);
					//flip_false_zero(temp_diff_XMM);
					temp_diff_XMM = _mm_and_si128(temp_diff_XMM, temp_mask);//11

					p_cur[j] = _mm_and_si128(p_up[j-1], temp_diff_XMM);
				}
				else
				{
					p_cur[j]=_mm_and_si128(p_up[j], p_up[j-1]);
				}
					total_difference = popcnt_sse_u64(p_cur[j]);
					if(total_difference<=max_error - i)
					{
						return 1;
					}
			}
		p_swap=p_up;
		p_up=p_cur;
		p_cur=p_swap;
		}

	}
	return 0;
}

int bit_vec_filter_sse1_ex(char* read, char* ref, int read_len, int ref_len, int max_error) {
	//Get ready the bits
	sse3_convert2bit1(read, read_vec0_t, read_vec1_t);
	sse3_convert2bit1(ref, ref_vec0_t, ref_vec1_t);

	return bit_vec_filter_m128_sse1_ex(read_vec0_t, read_vec1_t,
					ref_vec0_t, ref_vec1_t, read_len, ref_len, max_error);
}
